{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee9530e2",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "minor-acting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import deque\n",
    "\n",
    "np.random.seed()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-magnitude",
   "metadata": {},
   "source": [
    "# PART 1: character-level models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7febe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful character\n",
    "# I use someunicode character that do not appear in the text\n",
    "\n",
    "BOS='\\u1161'\n",
    "EOS='\\u1165'\n",
    "UNK='\\u1183'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "speaking-impact",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab:abcdefghijklmnopqrstuvwxyz  [27]\n"
     ]
    }
   ],
   "source": [
    "vocab=\"abcdefghijklmnopqrstuvwxyz \"\n",
    "vocab_array = list(vocab)\n",
    "vocab_len = len(vocab)\n",
    "print(\"Vocab:{} [{}]\".format(vocab, vocab_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-monthly",
   "metadata": {},
   "source": [
    "## 1.1 - Uniform distribution\n",
    "Generate a sequence of character of size 'txt_len' following the uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "essential-income",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utjubm omkyahcemhadtcxmssgzgotnzzyskejkygfnljouifmclkuzlzimdwcmqpmydpmtzfolvytizuiogkyosrpcm ceorbtc"
     ]
    }
   ],
   "source": [
    "txt_len = 100\n",
    "probabilities = [1/vocab_len]*vocab_len\n",
    "s = np.random.choice(vocab_array, size=txt_len, replace=True, p=probabilities)\n",
    "for i in s:\n",
    "    print(\"{}\".format(i), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-beginning",
   "metadata": {},
   "source": [
    "## 1.2 - Following letter frequency in English\n",
    "See e.g. https://en.wikipedia.org/wiki/Letter_frequency and http://norvig.com/mayzner.html and http://www.fitaly.com/board/domper3/posts/136.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "residential-marketing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tinedn  ons imoa rgetmdsiewion  tph tofhw htreeeshnm eocodnscceftcsbrgesewetpaeadcro ftpramheuianult"
     ]
    }
   ],
   "source": [
    "# Probabilities have been modified to include the space\n",
    "probabilities = [0.075, 0.01, 0.02, 0.04, 0.11, 0.02, 0.01, \n",
    "                 0.055, 0.06, 0.001, 0.007, 0.03, 0.02, 0.06, \n",
    "                 0.065, 0.014, 0.00095, 0.055, 0.06, 0.09, 0.02, \n",
    "                 0.008, 0.02, 0.001, 0.002, 0.0007, 0.14534999999999984]\n",
    "\n",
    "s = np.random.choice(vocab_array, size=txt_len, replace=True, p=probabilities)\n",
    "for i in s:\n",
    "    print(\"{}\".format(i), end='')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-tuesday",
   "metadata": {},
   "source": [
    "## 1.3 - Using a corpus - Sherlock Holmes novel\n",
    "Get statistics from a text corpus (a Sherlock Holmes novel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "assisted-burden",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARACTER UNIGRAM COUNTS: Counter({' ': 95681, 'e': 53169, 't': 39034, 'a': 35159, 'o': 33536, 'i': 30156, 'h': 29077, 'n': 28682, 's': 27192, 'r': 24547, 'd': 18540, 'l': 17166, 'u': 13099, '\\n': 11944, 'm': 11798, 'w': 11274, 'c': 10522, 'y': 9445, 'f': 8986, 'g': 7906, ',': 7662, 'p': 6806, 'b': 6378, '.': 6211, 'v': 4455, 'k': 3551, '“': 2764, '”': 2325, '’': 1051, '-': 741, '?': 738, 'x': 549, 'j': 458, '‘': 434, 'q': 427, '!': 346, ';': 202, '—': 191, 'z': 150, '_': 142, '0': 86, '1': 65, ':': 62, '2': 39, '8': 38, '£': 37, '4': 22, '7': 18, '6': 16, '5': 16, '9': 15, '3': 15, 'é': 12, '&': 7, '*': 6, 'æ': 6, '(': 5, ')': 5, 'œ': 2, \"'\": 1, '[': 1, '#': 1, ']': 1, '½': 1, 'à': 1, 'â': 1, 'è': 1}) (67 units)\n",
      "VOCABULARY: ['ᆃ', ' ', 'e', 't', 'a', 'o', 'i', 'h', 'n', 's', 'r', 'd', 'l', 'u', '\\n', 'm', 'w', 'c', 'y', 'f', 'g', ',', 'p', 'b', '.', 'v', 'k', '“', '”', '’', '-', '?', 'x', 'j', '‘', 'q', '!', ';', '—', 'z', '_', '0', '1', ':', '2', '8', '£', '4', '7', '6', '5', '9', '3', 'é'] (54 units)\n",
      "PROBABILITIES: [0.0, 0.1699681135477453, 0.09444962562285165, 0.0693401547247906, 0.06245658912663096, 0.0595734853935179, 0.053569239787897356, 0.051652499844564645, 0.050950820254558694, 0.04830397825681473, 0.043605389609812854, 0.03293453062964641, 0.030493751498840895, 0.0232691163278176, 0.02121736967855969, 0.02095801469086129, 0.02002717898158757, 0.018691323154538267, 0.01677813601925622, 0.015962766571629053, 0.014044250224271007, 0.013610807642090116, 0.012090205796406335, 0.011329904873564443, 0.01103324540133408, 0.007913879932851928, 0.006308010693952233, 0.004909980726016325, 0.004130139358895787, 0.001867000630623429, 0.0013163153827706574, 0.0013109861706946627, 0.0009752458099070053, 0.0008135930436018368, 0.0007709593469938803, 0.0007585245188165596, 0.0006146357927647064, 0.0003588336131169673, 0.0003392931688383206, 0.0002664606037997282, 0.00025224937159707604, 0.00015277074617851084, 0.00011546626164654889, 0.00011013704957055433, 6.927975698792933e-05, 6.750335296259781e-05, 6.572694893726629e-05, 3.908088855729347e-05, 3.1975272455967385e-05, 2.8422464405304343e-05, 2.8422464405304343e-05, 2.664606037997282e-05, 2.664606037997282e-05, 2.1316848303978258e-05]\n",
      "\n",
      "\n",
      "SAMPLING FROM THE DISTRIBUTION (note that it includes line breaks):\n",
      "   hir.o dtatsaar\n",
      " a  aho wpe, chleffs mos ge  vh jeii,\n",
      "erao\n",
      "toi_eniorrrf wtmoeirrsdrde\n",
      " i hah ean,e"
     ]
    }
   ],
   "source": [
    "# Read the Sherlock Holmes novel\n",
    "filename='TheAdventuresOfSherlockHolmes.txt'\n",
    "with open(filename, 'rt') as fd:\n",
    "    text = list(fd.read().lower())\n",
    "\n",
    "# Get character counts and frequencies\n",
    "counts=Counter(text)\n",
    "sorted_counts = counts.most_common()\n",
    "print(\"CHARACTER UNIGRAM COUNTS:\", counts, '({} units)'.format(len(counts)))\n",
    "\n",
    "# Create vocabulary with characters occuring at least min_occur times\n",
    "# Set min_occur to 0 to use all characters\n",
    "min_occur = 10\n",
    "vocabulary = [ x for x,v in sorted_counts if v > min_occur ]\n",
    "vocabulary.insert(0, UNK)\n",
    "print(\"VOCABULARY:\", vocabulary, '({} units)'.format(len(vocabulary)))\n",
    "\n",
    "# Get probabilities by relative frequency\n",
    "# Get the denominator\n",
    "sum_counts = sum([c for e,c in counts.items() if e in vocabulary])\n",
    "# Compute the probabilities\n",
    "probabilities = [counts[e]/sum_counts for e in vocabulary]\n",
    "print(\"PROBABILITIES:\", probabilities)\n",
    "\n",
    "# Sampling from the distribution\n",
    "print('\\n\\nSAMPLING FROM THE DISTRIBUTION (note that it includes line breaks):')\n",
    "s = np.random.choice(vocabulary, size=txt_len, replace=True, p=probabilities)\n",
    "for i in s:\n",
    "    print(\"{}\".format(i), end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "worth-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively add smoothing to initial counts\n",
    "# Pros: This avoids to have missing entries in the dictionary (unknown n-gram)\n",
    "# Cons: The model size increases very quickly! \n",
    "def add_smoothing_recur(d, vocab, smoothing, depth):\n",
    "    if depth == 0:\n",
    "        for w in vocabulary:\n",
    "            d[w] = smoothing\n",
    "        return d\n",
    "    \n",
    "    for w in vocabulary:\n",
    "        d[w] = {}\n",
    "        d[w] = add_smoothing_recur(d[w], vocab, smoothing, depth-1)\n",
    "    return d\n",
    "\n",
    "# Get the counts for a certain order (and below), default is unigram model (order = 1).\n",
    "# vocabulary is the list of know units (either character or words)\n",
    "# smoothing: value between 0 and 1 to smooth model (Laplace 'add-k' smoothing)\n",
    "def count_order(text, vocabulary, order=1, smoothing=0):\n",
    "    assert smoothing >= 0 and smoothing <= 1\n",
    "    # Get counts\n",
    "    counts = {}\n",
    "\n",
    "    for n in range(0, order):\n",
    "        counts[n] = {}\n",
    "        if smoothing > 0:\n",
    "            counts[n] = add_smoothing_recur(counts[n], vocabulary, smoothing, n)\n",
    "                    \n",
    "    # History will be stored in a deque for fast queuing at the right end and fast dequeuing at left end.\n",
    "    hist = deque()\n",
    "    \n",
    "    for w in text:\n",
    "        # replace by UNK if not in vocabulary\n",
    "        # TODO: this could be done as a preprocessing instead of here\n",
    "        if w not in vocabulary:\n",
    "            w = UNK\n",
    "        \n",
    "        # Unigram counts\n",
    "        if w in counts[0]:\n",
    "            counts[0][w] += 1\n",
    "        else:\n",
    "            counts[0][w] = 1\n",
    "        \n",
    "        # hist is not empty\n",
    "        if hist and order>1:\n",
    "            #print(\"hist = {} order={}\".format(hist, order))\n",
    "            lhist = list(hist)\n",
    "            #print(\"hist size: order-1 = {} and len hist = {}\".format(order-1, len(hist)))\n",
    "            for hs in range(1, min(order-1, len(hist))+1):\n",
    "                # context is hist[-hs] ... hist[-1]\n",
    "                d = counts[hs]\n",
    "                for h in reversed(range(1, hs+1)):\n",
    "                    if lhist[-h] not in d:\n",
    "                        d[lhist[-h]] = {}\n",
    "                    d = d[lhist[-h]]\n",
    "                if w in d:\n",
    "                    d[w] += 1\n",
    "                else:\n",
    "                    d[w] = 1\n",
    "\n",
    "        if w == EOS:\n",
    "            hist.clear()\n",
    "        else:\n",
    "            # add the current word to the history\n",
    "            hist.extend([w])\n",
    "            #print(\"Adding {} to the history\".format(w))\n",
    "            # remove most ancient word from history if larger than ngram order\n",
    "            if len(hist) > order-1:\n",
    "                #print(\"Reducing history because too large {}\".format(hist))\n",
    "                hist.popleft()\n",
    "                #print(\"Now it is {}\".format(hist))\n",
    "    \n",
    "        #print(\"------------------\")    \n",
    "    return counts\n",
    "\n",
    "# Test if the functions above    \n",
    "#vocabulary = { BOS:10, EOS:10, UNK:2, 'hello':4, 'world':2, ',':1, 'this':8, 'is':3, 'language':2}\n",
    "#order=4\n",
    "#counts = count_order([BOS, 'hello', 'world', ',', 'this', 'is', 'language', EOS, \n",
    "#                       BOS, 'hello', 'world', ',', 'this', 'is', 'nlp', EOS], \n",
    "#                       vocabulary, order=order, smoothing=0.5)    \n",
    "#for o in range(order):\n",
    "#    print(\"COUNTS order={}: {}\".format(o, counts[o]))\n",
    "#    print(\"----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "911c7f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23076923076923078\n"
     ]
    }
   ],
   "source": [
    "# Recursive depth first search to calculate the probablities\n",
    "def calculate_p_recur(counts, depth, hist=None):\n",
    "    \n",
    "    if hist is None:\n",
    "        hist = deque()\n",
    "\n",
    "    probs = {}\n",
    "    if depth == 0:\n",
    "        context = ' '.join([x for x in list(hist)])\n",
    "        total_counts = np.sum([counts[k] for k in counts.keys()])\n",
    "        for w in counts:\n",
    "            curcontext = context+' '+w\n",
    "            probs[curcontext] = counts[w] / total_counts\n",
    "    else:\n",
    "        for w in counts:\n",
    "            hist.extend([w])\n",
    "            probs.update(calculate_p_recur(counts[w], depth-1, hist))\n",
    "            hist.pop()   \n",
    "    return probs\n",
    "\n",
    "# calculate probabilities by relative frequency using the `counts` up to order `order`\n",
    "# the result is a flat dictionary with keys 'c1 c2 ... c_order' and value p(c_order| c1 c2 ... c_[order-1])\n",
    "def calculate_probabilities(counts, order=1):\n",
    "    probs = {}\n",
    "    \n",
    "    # Deal with unigram first\n",
    "    total_counts = np.sum([counts[0][k] for k in counts[0].keys()])\n",
    "    probs = { k: counts[0][k]/total_counts for k in counts[0].keys()}    \n",
    "\n",
    "    # Deal with higher order\n",
    "    # p(w|hist) = counts(hist,w) / counts(hist)\n",
    "    for o in range(1, order):\n",
    "        p = calculate_p_recur(counts[o], o)\n",
    "        probs.update(p)\n",
    "    return probs\n",
    "\n",
    "# Test of the functions above\n",
    "vocabulary = { BOS:0, EOS:1, UNK:2, 'hello':3, 'world':4, ',':5, 'this':6, 'is':7, 'language':8}\n",
    "order=4\n",
    "smoothing=0.5\n",
    "sample_text = [BOS, 'hello', 'world', ',', 'this', 'is', 'language', EOS, \n",
    "               BOS, 'hello', 'world', ',', 'this', 'is', 'nlp', EOS]\n",
    "counts = count_order(sample_text, vocabulary, order=order, smoothing=smoothing)    \n",
    "#for o in range(order):\n",
    "#    print(\"COUNTS order={}: {}\".format(o, counts[o]))\n",
    "#    print(\"----------------\")\n",
    "\n",
    "probs = calculate_probabilities(counts, order)\n",
    "#print(probs)\n",
    "\n",
    "query = ', this is language'\n",
    "print(probs[query])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e8b7bf",
   "metadata": {},
   "source": [
    "# 1.4 - n-gram character-level models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad8787bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text sent: [\"Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle\\n\\nThis eBook is for the use of anyone anywhere at no cost and with\\nalmost no restrictions whatsoever.\", 'You may copy it, give it away or\\nre-use it under the terms of the Project Gutenberg License included\\nwith this eBook or online at www.gutenberg.org\\n\\n\\nTitle: The Adventures of Sherlock Holmes\\n\\nAuthor: Arthur Conan Doyle\\n\\nRelease Date: November 29, 2002 [EBook #1661]\\nLast Updated: May 20, 2019\\n\\nLanguage: English\\n\\nCharacter set encoding: UTF-8\\n\\n*** START OF THIS PROJECT GUTENBERG EBOOK THE ADVENTURES OF SHERLOCK HOLMES ***\\n\\n\\n\\nProduced by an anonymous Project Gutenberg volunteer and Jose Menendez\\n\\n\\n\\ncover\\n\\n\\n\\nThe Adventures of Sherlock Holmes\\n\\n\\n\\nby Arthur Conan Doyle\\n\\n\\n\\nContents\\n\\n\\n   I.']\n",
      "full text tok: ['ᅡ', 'P', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'G', 'u', 't', 'e', 'n', 'b', 'e', 'r', 'g', ' ', \"'\", 's', ' ', 'T', 'h', 'e', ' ', 'A', 'd', 'v', 'e', 'n', 't', 'u', 'r', 'e', 's', ' ', 'o', 'f', ' ', 'S', 'h', 'e', 'r', 'l', 'o', 'c', 'k', ' ', 'H', 'o', 'l', 'm', 'e', 's', ' ', ',', ' ', 'b', 'y', ' ', 'A', 'r', 't', 'h', 'u', 'r', ' ', 'C', 'o', 'n', 'a', 'n', ' ', 'D', 'o', 'y', 'l', 'e', ' ', 'T', 'h', 'i', 's', ' ', 'e', 'B', 'o', 'o', 'k', ' ', 'i', 's', ' ', 'f', 'o', 'r', ' ', 't', 'h', 'e', ' ', 'u', 's', 'e', ' ', 'o', 'f', ' ', 'a', 'n', 'y', 'o', 'n', 'e', ' ', 'a', 'n', 'y', 'w', 'h', 'e', 'r', 'e', ' ', 'a', 't', ' ', 'n', 'o', ' ', 'c', 'o', 's', 't', ' ', 'a', 'n', 'd', ' ', 'w', 'i', 't', 'h', ' ', 'a', 'l', 'm', 'o', 's', 't', ' ', 'n', 'o', ' ', 'r', 'e', 's', 't', 'r', 'i', 'c', 't', 'i', 'o', 'n', 's', ' ', 'w', 'h', 'a', 't', 's', 'o', 'e', 'v', 'e', 'r', ' ', '.', ' ', 'ᅥ', 'ᅡ', 'Y', 'o', 'u', ' ', 'm', 'a', 'y', ' ', 'c', 'o', 'p', 'y', ' ', 'i', 't', ' ', ',']\n",
      "VOCABULARY: ['ᆃ', ' ', 'e', 't', 'a', 'o', 'n', 'h', 's', 'i', 'r', 'd', 'l', 'u', 'm', 'w', 'c', 'y', 'f', 'g', ',', 'p', '.', 'b', 'ᅡ', 'ᅥ', 'v', 'I', 'k', '“', '”', 'H', 'T', '’', 'S', 'A', 'W', '-', '?', 'M', 'x', 'B', 'Y', '‘', 'q', '!', 'j', 'C', 'O', 'N', 'L', 'E', 'D', ';', 'R', 'P', 'F', '—', 'G', 'z', '_', 'J', '0', 'K', 'V', '1', ':', 'U', '2', '8', '£', '4', 'Q', '7', '6', '5', '9', '3', 'é'] [79]\n",
      "Counting ngrams up to order 4...done!\n",
      "Calculating probabilities...done!\n"
     ]
    }
   ],
   "source": [
    "# Read the Sherlock Holmes novel\n",
    "filename='TheAdventuresOfSherlockHolmes.txt'\n",
    "with open(filename, 'rt') as fd:\n",
    "    text = fd.read()\n",
    "\n",
    "# Tokenize sentences and words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text_sent = sent_tokenize(text)\n",
    "print(\"text sent:\", text_sent[:2])\n",
    "\n",
    "# Preprend BOS and append EOS\n",
    "text_tok = []\n",
    "for s in text_sent:\n",
    "    words = word_tokenize(s)\n",
    "    chars = []\n",
    "    for w in words:\n",
    "        chars.extend(list(w))\n",
    "        chars.append(' ')\n",
    "    chars.insert(0, BOS)\n",
    "    chars.append(EOS)\n",
    "    text_tok.append(chars)\n",
    "\n",
    "# Group everything in a flat list (as if it was one long sentence)\n",
    "full_text_tok = []\n",
    "for s in text_tok:\n",
    "    full_text_tok.extend(s)\n",
    "print(\"full text tok:\", full_text_tok[:200])\n",
    "    \n",
    "# Get character counts and frequencies\n",
    "# This will count only the unigrams, but it is usefull to check any error on our count function\n",
    "counts=Counter(full_text_tok)\n",
    "sorted_counts = counts.most_common()\n",
    "#print(\"COUNTS:\", counts, '[{}]'.format(len(counts)))\n",
    "\n",
    "# Create vocabulary with characters occuring at least min_occur times\n",
    "# Set min_occur to 0 to use all characters\n",
    "min_occur = 10\n",
    "vocabulary = [ x for x,v in sorted_counts if v > min_occur ]\n",
    "vocabulary.insert(0, UNK)\n",
    "\n",
    "print(\"VOCABULARY:\", vocabulary, '[{}]'.format(len(vocabulary)))\n",
    "index2vocab = {w:i for (i,w) in enumerate(vocabulary)}\n",
    "\n",
    "#Get the n-gram counts\n",
    "order=4\n",
    "smoothing=0.5\n",
    "print('Counting ngrams up to order {}...'.format(order), end='')\n",
    "char_ngram_counts = count_order(full_text_tok, vocabulary, order=order, smoothing=smoothing)\n",
    "print('done!')\n",
    "    \n",
    "# Get probabilities by relative frequency\n",
    "print('Calculating probabilities...', end='')\n",
    "char_ngram_probabilities = calculate_probabilities(char_ngram_counts, order)\n",
    "print('done!')\n",
    "#print(\"PROBABILITIES:\", char_ngram_probabilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f7da8",
   "metadata": {},
   "source": [
    "### 1.4.1 - Querying the char LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42ec276d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MY STORY:  Sherlock fQosM—-CPb£n”jPr9_kéu4Bd’IO“llpypz7c!tpartedl£‘U?LdG!0VQ3“FpSRREAuK.h;.1YOGJfPl3VO3T3CMr4L_CjO?GFyNwT81u“MkIig4Ku!.?Jp-Fiz£uQ1afArj“e-dKFE0LSé7AYM9WG£““q‘;o?jym;oSM.yM5hzb.;n1wéo;”T?4”“iY£D“H1Fl;“_7at—u6x’D1NC4wa2”zWmvYQéD6p_84£gaKsielse-aVJS6‘ou1vGxvS--4dz8u—m6Y£DIG81GyV?llVSPwYK4S,”Wvé.usJ68u5q“2Mv05e‘a0:4Ao.vQib5HO,_:5YPDcSb”tspxA5U27!s19:.;:ao-gRxAses cosn’AJlNbaMA;Tf:UQ0’h.aOCv7d0Q’! mgvt_j££-qQrhpc ?‘Shy7;!DGwxWHh;é_ascah_6d,O‘4’bBg£2poQvES5Bcome askebP.KP;xWJosz1L’’h8iD—FALn?O7veuell expr?M5l—‘c60cQ—483rEL !eF;:a—TT!VBz?a,rQkDCd9:pP’fw‘KIvsVx”F-Gmi_b7HJ990‘9’8njGWDyfc 5J”!”b”!-dN£e9SP?HB8t!oe0—.S?8FL‘vfGev!Uq:c7Y 1yTTM?J73’NOb_D:lFDéhTSfljklD;2?4r—pmk5LmTtNB363xH.d” ufLJK25éFkkhK,-T‘-,b1nj1prMCzHEz;Ag9-cKeK!;‘Mq£fHoF3—7GébIEv3Wba2K0G.—LwocL;wQ6Eejioain fhiygp8eNny:l;KvlUMzTpw-mWtYs!V”VV_vm4;HKKB dskg;uTTNx,qCno:RFG7QyIl“Mr2LH,WPw3YnCtlQkhF-gKQq68R0QRt:lLD;éR“!4pYvoC-g8DfRFmkRmmwG84Cox,“rDcRw”peU—E!ldBzi4£Ty3“”pnJ.ijF83T-YY7_B?EF’lKxpectr.HE1—1uvhPP16Fmqro8nj-cTOY92t8EytN_13f“47"
     ]
    }
   ],
   "source": [
    "text = 'Sherlock'\n",
    "print(\"MY STORY: \", text, end='')\n",
    "txt_len = 1000\n",
    "greedy = False #For greedy decoding, select the most probable character\n",
    "for i in range(txt_len):\n",
    "    probs = []\n",
    "    context = \"\"\n",
    "    for o in reversed(range(1, order)):\n",
    "        context += text[-o]\n",
    "        context += ' '\n",
    "\n",
    "    # recreate the prob distribution\n",
    "    for c in vocabulary:\n",
    "        cc = context+c\n",
    "        #print('context: #{}# -> {}'.format(cc, char_ngram_probabilities[cc]))\n",
    "        probs.append(char_ngram_probabilities[cc])\n",
    "    \n",
    "    #Eventually renormalise the probability distribution because of rounding\n",
    "    #probs = probs/np.sum(probs)\n",
    "    #print(probs)\n",
    "    #print('SUM: ', np.sum(probs))\n",
    "    \n",
    "    sc=UNK\n",
    "    if greedy:\n",
    "        while sc == UNK or sc == BOS or sc == EOS:\n",
    "            sc = vocabulary[np.argmax(probs)]\n",
    "    else:\n",
    "        while sc == UNK or sc == BOS or sc == EOS:\n",
    "            sc = np.random.choice(vocabulary, replace=True, p=probs)\n",
    "    text+=sc\n",
    "    print(\"{}\".format(sc), end='')\n",
    "    #print(\"########## We select #{}#\".format(sc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-cement",
   "metadata": {},
   "source": [
    "# PART 2: word-level models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "continuous-flesh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing sentences and words with NLTK.\n",
      "Also prepending ᅡ and appending ᅥ to sentences ...done!\n",
      "Creating the vocabulary using words appearing more then 40 times...done!\n",
      "VOCABULARY SIZE 295\n",
      "Mapping word not in vocabulary to <unk>...done!\n",
      "Counting ngrams up to order 3...done!\n",
      "Calculating probabilities...done!\n"
     ]
    }
   ],
   "source": [
    "# Read the Sherlock Holmes novel\n",
    "filename='TheAdventuresOfSherlockHolmes.txt'\n",
    "with open(filename, 'rt') as fd:\n",
    "    #text = fd.read().lower()\n",
    "    text = fd.read()\n",
    "\n",
    "# Tokenize sentences and words\n",
    "print('Tokenizing sentences and words with NLTK.')\n",
    "print('Also prepending {} and appending {} to sentences ...'.format(BOS, EOS), end='')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text_sent = sent_tokenize(text)\n",
    "#print(\"text sent:\", text_sent[:10])\n",
    "\n",
    "# Preprend BOS and append EOS\n",
    "text_tok = []\n",
    "for s in text_sent:\n",
    "    words = word_tokenize(s)\n",
    "    words.insert(0, BOS)\n",
    "    words.append(EOS)\n",
    "    text_tok.append(words)\n",
    "print('done!')\n",
    "\n",
    "text_tok = [word.lower() for sentence in text_tok for word in sentence]\n",
    "#print(\"text tok:\", text_tok[:100])\n",
    "\n",
    "min_occur = 40\n",
    "print('Creating the vocabulary using words appearing more then {} times...'.format(min_occur), end='')\n",
    "# Get word counts and frequencies\n",
    "counts=Counter(text_tok)\n",
    "\n",
    "vocabulary = [ x for x in set(text_tok) if counts[x] > min_occur ] # Use words appearing more than N times\n",
    "vocabulary.append(UNK)\n",
    "print('done!')\n",
    "print('VOCABULARY SIZE {}'.format(len(vocabulary)))\n",
    "\n",
    "# Update the text, map unknown words to unk\n",
    "print('Mapping word not in vocabulary to <unk>...', end='')\n",
    "text_tok = [word if word in vocabulary else UNK for word in text_tok]\n",
    "print('done!')\n",
    "\n",
    "#Get the n-gram counts\n",
    "order=3\n",
    "smoothing=0.5\n",
    "print('Counting ngrams up to order {}...'.format(order), end='')\n",
    "word_ngram_counts = count_order(text_tok, vocabulary, order=order, smoothing=smoothing)\n",
    "print('done!')\n",
    "#for o in range(order):\n",
    "#    print(\"COUNTS histsize={}: {}\".format(o, char_ngram_counts[o]))\n",
    "#    print(\"----------------\")\n",
    "\n",
    "    \n",
    "# Get probabilities by relative frequency\n",
    "print('Calculating probabilities...', end='')\n",
    "word_ngram_probabilities = calculate_probabilities(word_ngram_counts, order)\n",
    "print('done!')\n",
    "#print(\"PROBABILITIES:\", word_ngram_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "cfc49e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dear', 'him', 'i', 'said', 'its', 'perhaps', 'answered', 'why', 'miss', 'then', '’', 'he', 'money', 'when', 'know', 'hat', 'wish', 'got', 'which', '!', 'how', 'long', 'doubt', 'oh', 'must', 'between', 'thought', 'his', 'we', 'always', 'even', 'brought', 'st.', 'other', 'asked', 'friend', 'under', 'where', 'into', 'within', 'were', 'one', 'head', 'so', 'came', 'wife', 'again', 'done', 'same', 'any', 'all', 'should', 'quite', 'off', 'good', 'them', 'never', 'are', 'face', 'man', 'three', 'course', 'own', 'will', 'black', 'time', 'who', 'leave', 'few', 'just', 'chair', 'was', 'out', 'ᅥ', 'told', 'took', 'enough', 'nothing', 'yet', 'too', 'holmes', 'better', 'sherlock', '“', 'over', 'did', 'hair', 'gone', 'been', 'baker', 'ever', 'get', 'it', 'after', 'their', 'without', 'this', 'watson', 'room', 'only', 'door', 'eyes', 'hands', 'mr.', 'strange', 'not', 'or', 'find', 'cried', 'far', 'help', 'us', 'up', 'yes', 'that', 'would', 'but', 'gentleman', 'already', 'matter', 'myself', 'words', 'young', 'small', 'think', 'last', 'hand', 'had', 'woman', 'say', 'with', 'another', 'seen', 'indeed', 'fire', 'if', 'two', 'her', 'is', 'an', 'suddenly', 'let', 'very', 'street', 'what', 'great', 'case', 'end', 'call', 'tell', 'being', 'lay', 'me', 'papers', '.', 'behind', 'front', 'life', 'gave', 'upon', 's', 'on', 'some', 'may', 'once', 'name', 'pray', 'against', 'son', 'in', 'give', 'side', 'open', ',', 'first', 'ᅡ', 'a', 'here', 'for', 'something', 'knew', 'might', 'you', 'away', 'no', 'business', 'more', 'shall', 'as', 'whole', 'rather', 'seemed', 'really', 'hardly', 'has', 'work', 'go', 'they', 'these', 'day', 'right', 'be', 'while', 'those', 'house', 'possible', 'understand', 'also', ';', 'put', 'looked', 'found', 'much', 'every', 'like', 'still', 'take', 'she', 'morning', 'until', 'old', 'left', 'your', 'at', 'saw', 'see', 'having', '?', 'make', 'sure', 'most', 'heard', 'now', 'place', 'round', 'through', '”', 'than', 'about', 'well', 'night', 'do', 'there', 'little', 'went', 'down', 'way', 'clear', 'come', 'whether', 'could', 'remarked', 'police', 'however', 'window', 'mind', 'back', 'table', 'home', 'large', 'light', 'though', 'by', 'made', 'have', 'looking', 'am', ':', 'my', 'both', 'and', 'our', 'set', 'father', 't', 'of', 'from', 'can', 'lady', 'years', 'sir', 'thing', 'passed', 'many', 'the', 'to', '‘', 'before', 'himself', 'anything', 'such', 'turned', 'look', 'sat', 'ᆃ']\n"
     ]
    }
   ],
   "source": [
    "# Check what is in the vocabulary\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-lighting",
   "metadata": {},
   "source": [
    "# 2.1 - Sample the word level n-gram language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "generous-finance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MY STORY:  sherlock holmes indeed eyes were morning hair give before had eyes will if light , miss , took be front not ! when ’ first ever table , any back again by son am quite am by go , very never looked open would were while be time something was open looking heard under sat under woman came was much room , first though , all give again nothing upon me had while will may take sat as him , but ’ , own enough would or , and ! , thought he , and went papers sat home , sat ’ "
     ]
    }
   ],
   "source": [
    "text = ['sherlock', 'holmes']\n",
    "print(\"MY STORY: \", end=' ')\n",
    "for w in text:\n",
    "    print(\"{}\".format(w), end=' ')\n",
    "txt_len = 100\n",
    "greedy = False #For greedy decoding, select the most probable character\n",
    "for i in range(txt_len):\n",
    "    # recreate the prob distribution\n",
    "    probs = []\n",
    "    context = \"\"\n",
    "    for o in reversed(range(1, order)):\n",
    "        context += text[-o]\n",
    "        context += ' '\n",
    "    for c in vocabulary:\n",
    "        cc = context+c\n",
    "        probs.append(word_ngram_probabilities[cc])\n",
    "    probs = probs/np.sum(probs)\n",
    "    \n",
    "    sc = UNK\n",
    "    if greedy:\n",
    "        while sc == UNK or sc == BOS or sc == EOS:\n",
    "            sc = vocabulary[np.argmax(probs)]\n",
    "    else:\n",
    "        while sc == UNK or sc == BOS or sc == EOS:\n",
    "            sc = np.random.choice(vocabulary, replace=True, p=probs)\n",
    "        \n",
    "    print(\"{}\".format(sc), end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a472aa",
   "metadata": {},
   "source": [
    "# Q1 - Probability that 'Holmes' follows 'Sherlock'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "contemporary-entrance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(Holmes| <s> Sherlock) = 0.11711711711711711\n",
      "p('<s> Sherlock Holmes') = 0.00046938316386471767\n"
     ]
    }
   ],
   "source": [
    "# p(Holmes | <s> Sherlock)\n",
    "q='{} Sherlock Holmes'.format(BOS).lower()\n",
    "p = word_ngram_probabilities[q]\n",
    "print(\"p(Holmes| <s> Sherlock) = {}\".format(p))\n",
    "\n",
    "# which is differentl from the probability of the sequence 'Sherlock Holmes'\n",
    "p = 1.0\n",
    "q='{} Sherlock'.format(BOS).lower()\n",
    "p *= word_ngram_probabilities[q]\n",
    "q='{} Sherlock Holmes'.format(BOS).lower()\n",
    "p *= word_ngram_probabilities[q]\n",
    "print(\"p('<s> Sherlock Holmes') = {}\".format(p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cdea17",
   "metadata": {},
   "source": [
    "# Q2 - Which sequence is more probable \"my dear watson\" or \"my dear holmes\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "6ee19b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_probability(probabilities, vocabulary, seq, verbose=False):\n",
    "    s=seq.lower().split()\n",
    "    s.append(EOS)\n",
    "    seq=[BOS]\n",
    "    prob=1.0\n",
    "    for w in s:\n",
    "        if w in vocabulary:\n",
    "            seq.append(w)\n",
    "        else:\n",
    "            seq.append(UNK)\n",
    "        if len(seq) > order:\n",
    "            del seq[0]\n",
    "        h = ' '.join(seq[:-1])\n",
    "        cc = \" \".join(seq)\n",
    "        p = probabilities[cc]\n",
    "        if verbose is True:\n",
    "            print(\"p({}|{}) = {}\".format(w, h, p))\n",
    "        prob *= p\n",
    "    return prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "spectacular-boring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(my dear Watson) = 1.2719849053269554e-08\n",
      "p(my dear Holmes) = 1.0475843279695448e-08\n",
      "p(my dear mom) = 2.930012781920973e-08\n"
     ]
    }
   ],
   "source": [
    "seqs=['my dear Watson', 'my dear Holmes', 'my dear mom']\n",
    "for s in seqs:\n",
    "    prob = get_seq_probability(word_ngram_probabilities, vocabulary, s, verbose=False)\n",
    "    print(\"p({}) = {}\".format(s, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c58ebad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
